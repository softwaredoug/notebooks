{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3e915c-c71b-4bf7-a379-49be6033d6bf",
   "metadata": {},
   "source": [
    "# Vector similarity search through shared reference points\n",
    "\n",
    "It was noted in [this blog post](https://softwaredoug.com/blog/2023/03/02/shared-dot-product.html) that if we know `u.A` and `v.A` we can estimate `u.v`. As an exercise, can we use that to prototype a vector similarity search?\n",
    "\n",
    "Why would this be useful?\n",
    "\n",
    "* We can compress a large vector space to a much reduced few thousand reference vectors, called `refs` here\n",
    "* We can index a set of vectors, `v`, by noting the most similar vectors to these `refs`, and storing the id and dot product `v.refs`\n",
    "* We might put that index in a traditional index like a search system, and just let traditional text retrieval's similarity work to create cosine similarity between dense vectors\n",
    "\n",
    "\n",
    "## Import wikipedia sentences and vectors\n",
    "\n",
    "Every 10 sentence of a wikipedia dump of sentences, totalling ~8m sentences/vectors. This is encoded with miniLM\n",
    "\n",
    "```\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    model.encode(sentence)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6894fa-6053-48b9-9de6-3566ad2ead2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(787183, 384)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('wikisent10.npz', 'rb') as f:\n",
    "    vects = np.load(f)\n",
    "    vects = np.stack(vects)\n",
    "    all_normed = (np.linalg.norm(vects, axis=1) > 0.99) & (np.linalg.norm(vects, axis=1) < 1.01)\n",
    "    assert all_normed.all(), \"Something is wrong - vectors are not normalized!\"\n",
    "\n",
    "with open('wikisent10.txt', 'rt') as f:\n",
    "    sentences = f.readlines()\n",
    "    \n",
    "vects.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d6fdd-c252-4977-8215-45a2dbdeee5f",
   "metadata": {},
   "source": [
    "## Ground truth similarity\n",
    "\n",
    "Take a dot product similarity to the query vector as the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "404772f4-3e6c-4aa1-bf7c-47725bec3115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 in the UK Singles chart, however it was a bigger hit for Amazulu in 1986 from their album Amazulu.\\n'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 100\n",
    "query_vector = vects[query]\n",
    "query_sentence = sentences[query]\n",
    "query_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b883def0-2c93-4e2d-89ea-1460850d20d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1.0 10 in the UK Singles chart, however it was a bigger hit for Amazulu in 1986 from their album Amazulu.\n",
      "\n",
      "349218 0.6210802 It was released as the album's fourth single in March 1986 and reached #15 on the Australian singles chart, becoming the band's tenth Top 20 hit in their country.\n",
      "\n",
      "545896 0.5978068 The album reached #18 in the UK Albums Chart and spawned two singles, both of which made the top 40 on the UK Singles Chart.\n",
      "\n",
      "324576 0.5959153 It reached number 16 in the UK Singles Chart in 1983, the band's biggest singles chart success prior to 1985.\n",
      "\n",
      "685786 0.5938797 The single was released on 13 March 1985 and entered the top 10 in Germany on 13 May 1985, after spending three weeks within the top-5, the single reached the top eventually going gold and selling well over 250,000 units in Germany alone.\n",
      "\n",
      "349159 0.5915458 It was released as a single in the UK in August 1985 where it reached number 74 in the singles charts and remained in the charts for 1 week.\n",
      "\n",
      "761042 0.58592045 Upon release, it peaked at number twenty on the UK Singles Chart.\n",
      "\n",
      "546282 0.58320194 The album sold over 200,000 copies and had six Top 20 singles, among them two number-one singles.\n",
      "\n",
      "685464 0.5829248 The single reached #83 in the UK Singles Chart.\n",
      "\n",
      "689124 0.58184844 The song reached 20 on the UK singles chart in June through August 1999, and had a second outing up to 16 on the US R&B chart in August 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn = np.dot(vects, query_vector)\n",
    "top_n = np.argpartition(-nn, 10)[:10]\n",
    "top_n = top_n[nn[top_n].argsort()[::-1]]\n",
    "\n",
    "for idx in top_n:\n",
    "    print(idx, nn[idx], sentences[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1332c7-8ee6-482e-bda8-915fb0df8795",
   "metadata": {},
   "source": [
    "## Select a set of random vectors as reference points\n",
    "\n",
    "We ensure we sample *randomly* otherwise the similarity below (summing similarities) won't work, as we'll be summing correlated similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fbb29811-bf51-48ff-8a2b-8334cf84d8be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.04418222e-02,  4.29905835e-02,  2.94846971e-02,  6.37413950e-02,\n",
       "        4.16853180e-03,  6.76388274e-02,  3.63467822e-02, -2.78905778e-02,\n",
       "        1.07696273e-01, -7.86593509e-02,  1.11071982e-01,  4.89706509e-02,\n",
       "       -1.09267697e-04,  2.96135333e-02,  2.32052021e-02, -4.47172151e-02,\n",
       "        6.61178519e-02, -1.61450268e-02, -1.38930192e-02, -6.20297679e-02,\n",
       "        6.61775297e-02, -2.21872601e-02, -3.58267370e-02,  3.20256973e-02,\n",
       "       -7.85287797e-02,  2.67882422e-02, -9.60284139e-02, -5.52966925e-02,\n",
       "        2.39018031e-02, -2.68202409e-02, -2.17792422e-02, -2.64472702e-02,\n",
       "        2.72490978e-02,  7.17630921e-02, -2.70215090e-02,  8.57689512e-02,\n",
       "        3.80243500e-02, -4.40758526e-02,  4.69071434e-03,  1.19036679e-02,\n",
       "       -1.88135365e-02,  4.26781438e-03,  1.50710805e-02, -7.08824250e-02,\n",
       "        4.15535225e-03, -1.82117029e-03, -4.75601741e-02,  3.18162279e-02,\n",
       "       -3.98048573e-02,  8.94637326e-02,  4.84469477e-02, -1.35898666e-02,\n",
       "        2.52745515e-02,  2.13522794e-02, -6.24090517e-02, -4.77663025e-02,\n",
       "        1.63446390e-02,  6.01218482e-02,  5.45837489e-02, -2.15819352e-03,\n",
       "        4.61829407e-02, -2.90004005e-02, -3.27905923e-03, -4.00509003e-02,\n",
       "       -4.30768917e-02,  2.55132413e-02, -8.44565679e-02,  3.35601592e-02,\n",
       "       -2.63518719e-02, -2.91544405e-02, -6.16691219e-02, -2.30064024e-02,\n",
       "       -4.66011586e-02,  5.94276977e-02, -3.76169295e-02, -3.33450854e-02,\n",
       "        3.11887457e-02,  1.32007441e-01, -9.89625061e-03,  1.00941216e-01,\n",
       "       -5.06599841e-02, -3.33290598e-02,  3.84441704e-04,  4.67506577e-02,\n",
       "        3.52920110e-02,  4.38606254e-02,  2.25823473e-04, -4.69938895e-02,\n",
       "       -2.69626960e-02,  1.23681500e-01, -5.60057816e-02,  4.43952060e-02,\n",
       "       -1.05460812e-01,  2.54565702e-02,  3.11364691e-02, -8.59580845e-03,\n",
       "       -8.53511071e-02, -5.02984876e-02, -1.00186782e-03, -8.07190266e-02,\n",
       "        6.79067800e-03, -1.19583079e-01,  1.45843911e-02, -3.08055643e-02,\n",
       "       -1.03239878e-01, -8.38485606e-03, -3.80857700e-02,  6.79612505e-03,\n",
       "        5.25052846e-02,  3.25733038e-02, -1.29127453e-03, -1.38967577e-01,\n",
       "        1.65654983e-02,  4.45227262e-03,  1.04210512e-04,  5.15959222e-02,\n",
       "        2.82520715e-02,  1.30457608e-01,  1.36443266e-02,  3.05271953e-02,\n",
       "       -2.01997681e-02, -3.05742263e-02,  1.16986148e-03, -3.63933065e-02,\n",
       "        4.76608945e-02,  1.76569886e-02, -2.55940454e-02, -7.10845249e-02,\n",
       "       -1.31460132e-02, -1.77426879e-02,  5.05254186e-02, -2.30482321e-02,\n",
       "        2.42092094e-02, -3.64509938e-02,  5.53035163e-02, -2.48722998e-02,\n",
       "        1.32087352e-02, -3.25946633e-02, -9.75569109e-03,  3.39032432e-02,\n",
       "       -8.93070868e-02,  8.63335098e-03,  5.35948776e-02, -2.67108613e-02,\n",
       "       -4.29501878e-02,  2.73160311e-02,  7.24727805e-04, -8.67175064e-02,\n",
       "        2.17943777e-02,  1.14403026e-01,  1.67830284e-03, -2.06097876e-02,\n",
       "        3.38756014e-02,  5.77731502e-02, -2.72068058e-02,  2.33582291e-02,\n",
       "        3.61819966e-03, -5.18335776e-02,  2.79581595e-02, -2.27397402e-02,\n",
       "        5.65724175e-02, -1.17043964e-02,  2.08731086e-02,  7.55550726e-02,\n",
       "        2.92342402e-02,  3.89012840e-02, -2.48447735e-02, -5.34542203e-02,\n",
       "        1.00968558e-01, -4.10726525e-03,  6.17329980e-03, -1.04402995e-01,\n",
       "        2.79259599e-02,  2.24982235e-02, -1.63822230e-02, -3.10028451e-02,\n",
       "        3.56055406e-02,  4.79054443e-02, -1.82553820e-03, -2.53142841e-02,\n",
       "        4.27241075e-02, -9.85995038e-02,  3.03154955e-02, -8.57015029e-03,\n",
       "        1.01329838e-01, -5.52095380e-02, -1.05697112e-01,  5.42081647e-02,\n",
       "       -2.73916915e-02, -2.48754932e-02, -5.36492928e-02, -7.15632541e-02,\n",
       "        4.05689417e-02,  1.31290402e-01, -4.98224336e-02,  5.97174178e-02,\n",
       "       -1.49611947e-02, -9.72509496e-02,  3.16467085e-02,  6.96245334e-02,\n",
       "       -8.57331799e-03,  2.78451122e-02,  3.20213244e-02,  4.20739420e-02,\n",
       "       -2.71539375e-02, -2.44101085e-02,  2.60139226e-02,  7.11493145e-03,\n",
       "        5.07575530e-02, -7.86363018e-02,  2.22944506e-02,  1.29516810e-02,\n",
       "        5.69221148e-02, -3.15771418e-02,  3.69435024e-02,  1.24824805e-02,\n",
       "       -5.05918417e-02,  2.60549003e-03,  8.08029228e-02, -1.93594587e-02,\n",
       "       -9.07600549e-02,  1.03479132e-02, -1.91972804e-02,  2.82372964e-02,\n",
       "       -4.47333957e-02,  1.32416619e-02,  6.73278323e-02, -1.08851173e-01,\n",
       "        4.62744432e-02,  4.84253562e-02, -2.50889608e-02,  3.62930100e-03,\n",
       "        3.81184399e-02,  7.60157122e-02, -2.12276909e-02,  2.28621680e-02,\n",
       "       -3.33051627e-02, -3.90953040e-02,  6.60735590e-02, -3.91903629e-02,\n",
       "        4.30199774e-02,  1.14109311e-01, -1.75843374e-02,  1.24243537e-02,\n",
       "       -7.25312293e-03, -4.16906140e-02, -1.39637889e-02, -2.54264600e-02,\n",
       "       -6.23869962e-02,  2.51084553e-03,  6.40034991e-02,  1.06636782e-02,\n",
       "        1.42581847e-02,  6.80277677e-03,  3.54875165e-03,  2.50765782e-02,\n",
       "       -3.58097287e-02, -1.79332983e-02,  1.02699316e-01, -1.44320928e-02,\n",
       "        3.03701805e-02,  1.53447904e-02, -1.00805457e-01, -4.92899037e-02,\n",
       "       -4.57628349e-03,  3.76260672e-02, -6.95361444e-02,  2.70601600e-03,\n",
       "        3.64168428e-02,  3.54973153e-02, -1.04052053e-02,  3.59951194e-03,\n",
       "       -7.21465069e-02, -1.14174848e-01, -9.35631222e-02,  3.46332099e-02,\n",
       "        2.64298931e-02, -3.61559819e-02, -4.37839426e-02, -1.07835055e-01,\n",
       "        4.14676736e-02,  5.68324765e-02,  5.14208425e-02, -8.32945492e-02,\n",
       "       -4.59030025e-02,  1.27615954e-01,  2.23258849e-02,  2.28798040e-03,\n",
       "       -1.83424039e-03, -6.64557493e-02, -5.93049516e-02, -4.20379274e-02,\n",
       "       -8.92106230e-02,  2.09542446e-03,  5.31381178e-02,  2.62957452e-02,\n",
       "       -9.06712591e-02,  2.38276498e-02, -3.54858078e-02,  4.60832007e-02,\n",
       "        2.67601141e-02,  6.97673102e-02,  3.45835306e-03, -3.13418384e-02,\n",
       "        2.17320521e-02,  2.72064288e-02, -2.94531959e-02,  3.55246778e-03,\n",
       "        4.34960744e-02,  5.45456454e-02, -5.56225124e-02,  3.87181179e-02,\n",
       "       -5.59771413e-02,  3.34249567e-03, -1.62459799e-02, -3.02944055e-02,\n",
       "        3.04816962e-02,  6.81297525e-02, -4.66034484e-02, -1.66771513e-02,\n",
       "        6.04135442e-02,  3.41091129e-02, -5.52885396e-02, -7.01254736e-04,\n",
       "       -6.72984894e-02, -7.41100675e-03, -1.83164083e-02,  1.75122920e-02,\n",
       "        2.41576192e-02, -2.76836051e-03,  3.26841186e-02,  8.77803568e-03,\n",
       "       -2.31833862e-02,  3.85738663e-02,  4.66707542e-02,  9.72742521e-04,\n",
       "       -5.40043204e-02,  1.10106366e-01,  1.07549882e-02, -5.03263457e-02,\n",
       "       -9.77505450e-02, -7.09567580e-02, -5.30466291e-02,  8.46738973e-02,\n",
       "       -1.45399960e-02,  5.77082350e-02, -9.54013816e-02,  2.54695420e-02,\n",
       "       -3.20152169e-02, -1.26542989e-02,  4.34733514e-02, -1.70223655e-02,\n",
       "        5.57654333e-03,  1.98642813e-03, -4.97658178e-02,  1.46301603e-03,\n",
       "       -4.09833888e-02, -7.89175370e-02,  9.17972094e-02, -1.01362564e-02,\n",
       "        4.37457414e-02,  1.27553353e-01, -1.25560804e-02, -1.30639167e-01,\n",
       "       -5.64585894e-02,  1.12000660e-02, -2.75970882e-02, -1.53966126e-02,\n",
       "       -6.64300537e-02, -2.39240883e-02,  3.81631119e-03,  1.28833991e-01,\n",
       "       -3.32094193e-02, -2.62501853e-02, -1.74170271e-02, -8.15272469e-02,\n",
       "        1.82607487e-02, -2.45952700e-02,  1.87688467e-02, -2.25024593e-02,\n",
       "        3.43321015e-02,  3.18052071e-02,  2.99881454e-02,  1.00104482e-01])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_vectors = len(vects)\n",
    "\n",
    "def centroid():\n",
    "    \"\"\" Sample a unit vector from a sphere in N dimensions.\n",
    "    It's actually important this is gaussian\n",
    "    https://stackoverflow.com/questions/59954810/generate-random-points-on-10-dimensional-unit-sphere\n",
    "    IE Don't do this\n",
    "        projection = np.random.random_sample(size=num_dims)\n",
    "        projection /= np.linalg.norm(projection)\n",
    "    \"\"\"\n",
    "    num_dims = len(vects[0])\n",
    "    projection = np.random.normal(size=num_dims)\n",
    "    projection /= np.linalg.norm(projection)\n",
    "    return projection   \n",
    "\n",
    "centroid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114e48c-90ba-4e04-8555-45449f2b5f61",
   "metadata": {},
   "source": [
    "## Most similar vectors to centroid\n",
    "\n",
    "Get most similar vectors, with a specified floor in specificity. The top 10 here should correspond to the top 10 ground truth above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "494f7e0a-d5a5-42c6-8154-e476c69f48a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 1.0),\n",
       " (349218, 0.6210802),\n",
       " (545896, 0.5978068),\n",
       " (324576, 0.5959153),\n",
       " (685786, 0.5938797),\n",
       " (349159, 0.5915458),\n",
       " (761042, 0.58592045),\n",
       " (546282, 0.58320194),\n",
       " (685464, 0.5829248),\n",
       " (689124, 0.58184844)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar(centroid, floor):\n",
    "\n",
    "    nn = np.dot(vects, centroid)\n",
    "    idx_above_thresh = np.argwhere(nn >= floor)[: ,0]\n",
    "\n",
    "    return sorted(zip(idx_above_thresh, nn[idx_above_thresh]),\n",
    "                key=lambda vect: vect[1],\n",
    "                reverse=True)\n",
    "\n",
    "nn_above_thresh = most_similar(query_vector, 0.001)\n",
    "nn_above_thresh[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf2f93-b5b7-4835-ab89-cc8e62bbd54b",
   "metadata": {},
   "source": [
    "## Create a compressed index based on shared reference points\n",
    "\n",
    "As mentioned in [this blog article](https://softwaredoug.com/blog/2023/03/02/shared-dot-product.html) we can use shared reference points between query and vector to estimate their similarity. Below we store\n",
    "\n",
    "- A table of these reference vectors (`refs`) that can stand in for the full vector space\n",
    "- A mapping of these `refs` -> a set of indexed vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e68ba0-04f7-454c-80c8-6083bebccca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 0.3291675839573145\n",
      "100 0.9029526806346173 43.776729874894954\n",
      "200 0.9919116647590205 88.12234558397904\n",
      "300 0.9993965824973354 132.4025780420052\n",
      "400 0.9999682411840702 176.65073737490457\n",
      "500 0.9999911075315396 220.59495983389206\n",
      "600 0.9999974592947256 264.75591770897154\n",
      "700 1.0 308.87807195889764\n",
      "800 1.0 352.69318049994763\n",
      "900 1.0 396.41712533391546\n",
      "1000 1.0 440.5252767499769\n",
      "1100 1.0 484.54106754192617\n",
      "1200 1.0 528.2496463749558\n",
      "1300 1.0 572.71075762494\n",
      "1400 1.0 616.7152932919562\n",
      "1500 1.0 661.9096008338965\n"
     ]
    }
   ],
   "source": [
    "ref_neighbors = {}   # reference pts -> neighbors\n",
    "from time import perf_counter\n",
    "\n",
    "num_refs = 2000\n",
    "refs = np.zeros( (num_refs, vects.shape[1]) )\n",
    "\n",
    "all_indexed_vectors = set()\n",
    "\n",
    "start = perf_counter()\n",
    "\n",
    "for ref_ord in range(0, num_refs):\n",
    "    specificity = 0.10\n",
    "    \n",
    "    center = centroid()    \n",
    "    top_n = most_similar(center, specificity)\n",
    "    \n",
    "    if ref_ord % 100 == 0:\n",
    "        print(ref_ord, len(set(all_indexed_vectors)) / len(vects), perf_counter() - start)\n",
    "        \n",
    "    refs[ref_ord, :] = center\n",
    "    idx = []\n",
    "    for vector_ord, dot_prod in top_n:\n",
    "        all_indexed_vectors.add(vector_ord)\n",
    "        idx.append((vector_ord, dot_prod))\n",
    "    ref_neighbors[ref_ord] = idx\n",
    "    if vector_ord == query:\n",
    "        print('Q', ref_ord, vector_ord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd73f9-a2e2-4859-968a-4a709f96a180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = np.dot(refs, query_vector)\n",
    "nn[nn > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c1bdc-d1b6-433c-a85c-2a1120bb8b5b",
   "metadata": {},
   "source": [
    "# Search time!\n",
    "\n",
    "Now when we search we go through the following steps, using just our reference points.\n",
    "\n",
    "## Similarity to reference points\n",
    "\n",
    "Compute similarity to `refs` from above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941af46-79c7-4bf3-805b-08bf2e886a97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query vect -> refs similarity\n",
    "nn = np.dot(refs, query_vector)\n",
    "\n",
    "top_n_ref_points = np.argpartition(-nn, 10)[:10]\n",
    "scored = nn[top_n_ref_points]\n",
    "\n",
    "scored, top_n_ref_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c22529-96ef-4aee-8123-c7ede47dea94",
   "metadata": {},
   "source": [
    "## Using reference points, As, estimate q.v\n",
    "\n",
    "We have query vector `q`, which is similar to a set of reference points `A`, can we estimate `q.v`. We expect `q.v` to [approach `q.A*v.A` as we implement below](https://softwaredoug.com/blog/2023/03/02/shared-dot-product.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c66f3750-255e-4d49-99f6-686f35d9f262",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(723905, [0.03571343463128332, 0.012700629623260861]),\n",
       " (503548, [0.03522915437314725]),\n",
       " (613207, [0.03332362985738621]),\n",
       " (743038, [0.03329816556012523]),\n",
       " (770521, [0.032652819867609234]),\n",
       " (665689, [0.03220477492385285]),\n",
       " (252842, [0.03208397976780289]),\n",
       " (755218, [0.031997149848823646]),\n",
       " (2046, [0.03154499853027196]),\n",
       " (552387, [0.03150694500555567])]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = {}\n",
    "cutoff = 0.0\n",
    "for ref_ord, ref_score in zip(top_n_ref_points, scored):\n",
    "    ref = refs[ref_ordinal]\n",
    "\n",
    "    for vect_id, score in ref_neighbors[ref_ord]:\n",
    "        # print(vect_id, score, score*ref_score)\n",
    "        combined = score * ref_score\n",
    "        if combined > cutoff:\n",
    "            try:\n",
    "                candidates[vect_id].append(combined)\n",
    "            except KeyError:\n",
    "                candidates[vect_id] = [combined]\n",
    "            \n",
    "list(candidates.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4f402-c454-4296-9f3d-cd84c615ed09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sum the shared candidates\n",
    "\n",
    "Should we sum the shared reference points?\n",
    "\n",
    "Out of N reference points A0...AN we observe `u.A0...u.AN` and `v.0...vN`. We assume `u.v` would correlate to the dot product of these `u.A0*v.A0 + u.A1*v.A1 + ... + u.AN*v.AN`.\n",
    "\n",
    "Note this only applies because we generate the reference points *randomly* introducing some bias in the reference points would create a case where many terms of the summation correlated heavily (the similarit yof `ref` `A0` and `A1` were so similar, that it was double counting). For example if `A0` and `A1` both occured towards the center of the data, we would be biased towards more general responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2a264261-2603-451a-b846-5cde37eeaa35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summed_candidates = {}\n",
    "for vect_id, scored in candidates.items():\n",
    "    summed_candidates[vect_id] = sum(scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8d181b23-2d50-4d5f-a8d4-1611d2946616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZF --  100 10 in the UK Singles chart, however it was a bigger hit for Amazulu in 1986 from their album Amazulu.\n",
      "\n",
      "0 (100, 0.16053842822469955) 10 in the UK Singles chart, however it was a bigger hit for Amazulu in 1986 from their album Amazulu.\n",
      "\n",
      "1 (707142, 0.1110197564947739) The track was released as a single, on EMI and reached 71 in the UK Singles Chart, spending just two weeks in the listing.\n",
      "\n",
      "2 (544532, 0.10054923103439038) The album debuted at #2 on the RIANZ albums chart, and after seven weeks within the top 10 would finally reach the #1 position.\n",
      "\n",
      "3 (323191, 0.09353954548607514) It peaked at number 10 and remained on the charts for 12 weeks, and it was also included on the Ice Age film.\n",
      "\n",
      "4 (690165, 0.09333054109079071) The song was originally released in February 1980, reaching #56 in the UK charts, before being re-released to top ten success in August of the same year.\n",
      "\n",
      "5 (545629, 0.09237140430525709) The album made its first appearance on Billboard magazine's album chart in the May 5, 1961, issue and reached number 38 during its 23 weeks there.\n",
      "\n",
      "6 (324529, 0.0916540870802981) It reached a peak position of number forty-two in the UK albums chart, and spent four weeks in France's album charts, peaking at number nine.\n",
      "\n",
      "7 (722420, 0.09112124142867335) They have had five number one albums in the UK, with their 1991 album, Stars, one of the best-selling albums in UK chart history.\n",
      "\n",
      "8 (428396, 0.09059988391522035) Number Ones was successful around the world, originally reaching number 1 in the UK and number 13 in the United States.\n",
      "\n",
      "9 (545604, 0.09047832812940113) The album itself reached #50 on the UK album charts on its initial release.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = summed_candidates.items()\n",
    "results = sorted(results,\n",
    "                 key=lambda scored: scored[1],\n",
    "                 reverse=True)[:10]\n",
    "# 21340\n",
    "print(\"ZF -- \", query, sentences[query])\n",
    "rank = -1\n",
    "for idx, result in enumerate(results):\n",
    "    print(idx, result, sentences[result[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9d8d1-52fb-4a68-a15e-2cb1b902d97e",
   "metadata": {},
   "source": [
    "## Putting search together\n",
    "\n",
    "Let's put the code above into one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f7497fd6-18ed-4b17-9c31-d8280a6327ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (478462, 0.14556285696774182) Robot9000 (r9k) is an open-source chat moderation script developed in 2008 by Randall Munroe.\n",
      "\n",
      "1 (566992, 0.13481037399967608) The chat show consisted of various games and quizzes presented towards celebrities who were guests on the episode, and began airing from 20 November 2013.\n",
      "\n",
      "2 (107215, 0.13402052882769422) Conversation theory is a cybernetic and dialectic framework that offers a scientific theory to explain how interactions lead to \"construction of knowledge\", or \"knowing\": wishing to preserve both the dynamic/kinetic quality, and the necessity for there to be a \"knower\".\n",
      "\n",
      "3 (107210, 0.13322484355276576) Conversation analysis (CA) is an approach to the study of social interaction, embracing both verbal and non-verbal conduct, in situations of everyday life.\n",
      "\n",
      "4 (169547, 0.13167264686562835) Google Talk (also known as Google Chat) is an instant messaging service that provides both text and voice communication.\n",
      "\n",
      "5 (489317, 0.13032269999154028) Semantic AI (not to be confused with Symantec), is a California C-corporation that offers patented, graph-based knowledge discovery, analysis and visualization software technology.\n",
      "\n",
      "6 (703364, 0.1277304945848734) The third-party extension support to iMessage meant it was \"becoming a platform\", although the user interface was criticized for being difficult to understand.\n",
      "\n",
      "7 (786907, 0.12734604820306397) Zo is an artificial intelligence English-language chatbot developed by Microsoft.\n",
      "\n",
      "8 (385560, 0.12640853910472064) Lingo is a verbose object-oriented (OO) scripting language developed by John H. Thompson for use in Adobe Director (formerly Macromedia Director).\n",
      "\n",
      "9 (325507, 0.12606332079486388) It rose to popularity in 1998, following the release of the first PC-based game in the series.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _search(query_vector):\n",
    "    \n",
    "    # query vect -> refs similarity\n",
    "    nn = np.dot(refs, query_vector)\n",
    "\n",
    "    top_n_ref_points = np.argpartition(-nn, 30)[:30]\n",
    "    scored = nn[top_n_ref_points]\n",
    "\n",
    "    # Candidates via our index\n",
    "    candidates = {}\n",
    "    cutoff = 0.0\n",
    "    for ref_ord, ref_score in zip(top_n_ref_points, scored):\n",
    "        ref = refs[ref_ordinal]\n",
    "\n",
    "        for vect_id, score in ref_neighbors[ref_ord]:\n",
    "            # print(vect_id, score, score*ref_score)\n",
    "            combined = score * ref_score\n",
    "            if combined > cutoff:\n",
    "                try:\n",
    "                    candidates[vect_id].append(combined)\n",
    "                except KeyError:\n",
    "                    candidates[vect_id] = [combined]\n",
    "                    \n",
    "    summed_candidates = {}\n",
    "    for vect_id, scored in candidates.items():\n",
    "        summed_candidates[vect_id] = sum(scored)\n",
    "        \n",
    "    results = summed_candidates.items()\n",
    "    return sorted(results,\n",
    "                  key=lambda scored: scored[1],\n",
    "                  reverse=True)[:10]\n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def search(query):\n",
    "    query_vector = model.encode(query)\n",
    "    return _search(query_vector)\n",
    "\n",
    "def search_ground_truth(query):\n",
    "    query_vector = model.encode(query)\n",
    "    nn = np.dot(vects, query_vector)\n",
    "    top_n = np.argpartition(-nn, 10)[:10]\n",
    "    top_n = top_n[nn[top_n].argsort()[::-1]]\n",
    "    return sorted(zip(top_n, nn[top_n]),\n",
    "                  key=lambda scored: scored[1],\n",
    "                  reverse=True)[:10]\n",
    "\n",
    "results = search(\"ai chatbots\")\n",
    "rank = -1\n",
    "for idx, result in enumerate(results):\n",
    "    print(idx, result, sentences[result[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f5fd8225-c1aa-4218-a1b2-2dcb98a14f64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (194492, 0.6189599) He is currently CEO and co-founder of marketing-tech company Botworx.ai an artificial intelligence (AI) and natural language processing company that aims to help businesses interact with their customers via AI-powered chatbots.\n",
      "\n",
      "1 (478462, 0.57323676) Robot9000 (r9k) is an open-source chat moderation script developed in 2008 by Randall Munroe.\n",
      "\n",
      "2 (701694, 0.565655) The term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs.\n",
      "\n",
      "3 (786907, 0.5553341) Zo is an artificial intelligence English-language chatbot developed by Microsoft.\n",
      "\n",
      "4 (331359, 0.529726) Its stated aim is to \"simulate natural human chat in an interesting, entertaining and humorous manner\".\n",
      "\n",
      "5 (730360, 0.51735985) This game is one of many simple games created by Google that are AI based as part of a project known as 'A.I. Experiments'.\n",
      "\n",
      "6 (64447, 0.5083422) A web chat is a system that allows users to communicate in real time using easily accessible web interfaces.\n",
      "\n",
      "7 (770238, 0.5072831) Web sites exist that publish chat logs, usually dedicated to a single channel and including a search engine.\n",
      "\n",
      "8 (743272, 0.5042565) This would be considered a chat room.\n",
      "\n",
      "9 (532386, 0.5000167) TekBots are programmable robots used by several universities to help students learn some of the fundamental concepts that are found in the fields of computer and electrical engineering.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = search_ground_truth(\"ai chatbots\")\n",
    "rank = -1\n",
    "for idx, result in enumerate(results):\n",
    "    print(idx, result, sentences[result[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca55be8-089f-40da-b268-1fcd111de991",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "This is just a toy prototype of course, and would need to be evaluated for recall.\n",
    "\n",
    "* Consider how you'd treat the reference points in a traditional search index (like Solr, Elasticsearch etc)\n",
    "* Benchmark with more data (9m -> 90m wikipedia sentences)\n",
    "* Study the relationship of needed reference points to get good recall\n",
    "* Test and ensure increasing reference points increases recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94cf913-6101-410d-857d-504c3bd002f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
